{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Virality prediction from tweet features.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0j0hWZZ16bL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45204a9e-d9bc-4720-a765-e0a1af64a111"
      },
      "source": [
        "import tweepy\n",
        "import json\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "# API access details - get yours from Twitter\n",
        "access_token = \"1236552411738595328-e1H9aw4mATD4piYGwNuSOQjU6r3M6C\"\n",
        "access_token_secret = \"6E8WAnFfYaNOw9vIFQoy6t5EGUsT2NcTX4T3C4DUkuqFu\"\n",
        "consumer_key = \"YiiFPmcIiRofzMQbzAW9KVER4\"\n",
        "consumer_secret = \"IE8knxJBbFLI2k1kTZMrzLcFpGA6hHB0zUorSwssNfKlx92aD3\"\n",
        "#Authenticate to Twitter api using our account keys\n",
        "auth_details = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth_details.set_access_token(access_token, access_token_secret)\n",
        " \n",
        "#Create connection to the Twitter api\n",
        "api = tweepy.API(auth_details)\n",
        "\n",
        "#Loop and sleep to download tweets\n",
        "for x in range(0,6):\n",
        "    tweet_array = []\n",
        "    tweets = api.home_timeline(count=100)\n",
        "    for tweet in tweets:\n",
        "        t = {\"text\" : tweet.text, \"likes\": tweet.favorite_count, \"retweets\": tweet.retweet_count }\n",
        "        tweet_array.append(t)\n",
        "        d = datetime.datetime.today().strftime('%Y-%m-%d-%H-%M-%s')\n",
        "        f = open('tweets-' + d + \".json\", 'w')\n",
        "        f.write(json.dumps(tweet_array))\n",
        "        f.close()\n",
        "    print(\"Sleeping\")\n",
        "    time.sleep(60 * 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sleeping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F04tCm_k3OCX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "84f8ab39-c274-45a0-c539-14f5ba40acf7"
      },
      "source": [
        "import glob\n",
        "import pathlib\n",
        "import os\n",
        "import json\n",
        "\n",
        "#Make sure we're in the same dir that has the downloaded JSON twitter data\n",
        "os.chdir('/content')\n",
        "\n",
        "tweets = []\n",
        "\n",
        "for file in glob.glob('*.json'):\n",
        "    with open(file) as json_data:\n",
        "        data = json.load(json_data)\n",
        "        for tweet in data:\n",
        "            tweets.append(tweet)\n",
        "        \n",
        "#Show how many tweets we've got\n",
        "print(len(tweets))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMZoM-DB3ksy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from operator import itemgetter\n",
        "# pre-processing of twitter data\n",
        "def prepareSentence(s):\n",
        "    stemmer = LancasterStemmer()\n",
        "    ignore_words = set(stopwords.words('english'))\n",
        "    regpattern = re.compile('[\\W_]+\" \"')\n",
        "    s = re.sub('[^A-Za-z ]+', '', s)\n",
        "    words = nltk.word_tokenize(s.lower())\n",
        "    return [stemmer.stem(w.lower()) for w in words if w not in ignore_words]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2RhNHVFKjvR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "23862a1c-3c20-466f-bf92-3797c6270c26"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "  "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeMX_JSS3qGO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "92db1c7d-fa8a-484a-898e-3fa0841fe332"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "words = []\n",
        "\n",
        "for tweet in tweets:\n",
        "    words.extend(prepareSentence(tweet[\"text\"]))\n",
        "\n",
        "distinct_words = set(words)\n",
        "\n",
        "lower_threshold = 10\n",
        "upper_threshold = 350\n",
        "counts = []\n",
        "final_words = []\n",
        "for word in distinct_words:\n",
        "    counts.append(words.count(word))\n",
        "    if words.count(word) > lower_threshold and words.count(word) < upper_threshold: \n",
        "        final_words.append(word)\n",
        "\n",
        "print(len(words))\n",
        "print(len(distinct_words))\n",
        "print(len(final_words))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "1039\n",
            "575\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AJ_Hjpb4O89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# deffination to append all unique words into bag of words\n",
        "def toBOW(sentance, words):\n",
        "    bag = []\n",
        "    for word in words:\n",
        "        bag.append(1) if word in sentance else bag.append(0)\n",
        "    return bag"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyZWv4Ff4TDM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e5e05457-66ef-4668-91de-2573044525db"
      },
      "source": [
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "for tweet in tweets:\n",
        "    sentence = prepareSentence(tweet[\"text\"])\n",
        "    # create our bag of words array\n",
        "    bag = toBOW(sentence, final_words)\n",
        "    inputs.append(bag)\n",
        "    #Calculate a score, 1 if any engagement, 0 if none\n",
        "    score = max(tweet[\"likes\"] + tweet[\"retweets\"],1)\n",
        "    \n",
        "    \n",
        "    outputs.append(score)\n",
        "\n",
        "#Define and train the network\n",
        "nnet = MLPRegressor(activation='relu', alpha=0.0001, hidden_layer_sizes=(int(len(final_words)*0.5),int(len(final_words)*0.25)),solver='adam', max_iter=400)\n",
        "nnet.fit(inputs, outputs)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "             hidden_layer_sizes=(3, 1), learning_rate='constant',\n",
              "             learning_rate_init=0.001, max_fun=15000, max_iter=400,\n",
              "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "             warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAD-Dn4e4nzC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "803e870b-b5ba-4c0d-b191-7b85df3ec94a"
      },
      "source": [
        "!pip install feedparser"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.6/dist-packages (5.2.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh_-Wj9_4YfR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "a370fdc7-5804-4a21-d678-083dded95bdd"
      },
      "source": [
        "\n",
        "potential_posts = {}\n",
        "\n",
        "for tweet in tweets:\n",
        "        sentence = prepareSentence(tweet[\"text\"])\n",
        "        bag = toBOW(sentence, final_words)\n",
        "        potential_posts[tweet[\"text\"]] = nnet.predict([bag])[0]\n",
        "    \n",
        "\n",
        "x = sorted(potential_posts.items(), key=itemgetter(1), reverse=True)\n",
        "\n",
        "print(\"\\nTop post 1:\\n\")\n",
        "for i in x[0]:\n",
        "    print(i)\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------\\n\")\n",
        "print(\"\\nTop post 2:\\n\")\n",
        "for i in x[1]:\n",
        "    print(i)\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------\\n\")\n",
        "print(\"\\nTop post 3:\\n\")\n",
        "for i in x[2]:\n",
        "    print(i)\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------\\n\")\n",
        "print(\"\\nTop post 4:\\n\")\n",
        "for i in x[3]:\n",
        "    print(i)\n",
        "\n",
        "print(\"-----------------------------------------------------------------------------------\\n\")\n",
        "print(\"\\nTop post 5:\\n\")\n",
        "for i in x[4]:\n",
        "    print(i)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Top post 1:\n",
            "\n",
            "@MoHFW_INDIA\n",
            " issues Revised Discharge Policy for confirmed #COVID19 cases...\n",
            "-----------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Top post 2:\n",
            "\n",
            "@drharshvardhan, along with Sh @AshwiniKChoubey, reviews #COVID19 management in eight states...\n",
            "-----------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Top post 3:\n",
            "\n",
            "Spitting in public can increase the risk of spread of #COVID19. The onus is on us. Be responsible, be safe...\n",
            "-----------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Top post 4:\n",
            "\n",
            "आइये, हम अपने स्वास्थ्य सेवाकर्मियों का ध्यान रखें। महामारी के दौरान उनके साथ किसी भी तरह की हिंसा या संपत्ति को नुकसान पहुंचाना एक दंडनीय अपराध है।...\n",
            "-----------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Top post 5:\n",
            "\n",
            "Indians stranded abroad due to the #COVID19 pandemic are getting back home, via air ✈️and sea...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}